{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "We sugesst to keep this notebook and work on a copy of this file that you can refer to this notebook whenever is necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- First import data and become familiar with the data. \n",
    "\n",
    "*To do so we should:* \n",
    "\n",
    "- Import required library \n",
    "\n",
    "- Import data set and become comftable with data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Some Data explanation is in order here **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism',\n",
    "              'talk.religion.misc',\n",
    "              'comp.graphics',\n",
    "              'sci.space']\n",
    "\n",
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=fetch_20newsgroups(subset='train', categories=categories,shuffle=True)\n",
    "test=fetch_20newsgroups(subset='test', categories=categories,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=dict()\n",
    "test_data=dict()\n",
    "\n",
    "train_data['target'] = train.target\n",
    "test_data['target'] = test.target\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "train_data['data'] = vectorizer.fit_transform(train.data)\n",
    "test_data['data'] = vectorizer.transform(test.data)\n",
    "\n",
    "with open('train.pkl','wb') as f0:\n",
    "    pickle.dump(train_data,f0)\n",
    "f0.close()\n",
    "with open('test.pkl','wb') as f0:\n",
    "    pickle.dump(test_data,f0)\n",
    "f0.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Data Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 33809)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33809"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dim = train_data['data'].shape[1]\n",
    "feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you start by applying LDA directely on the training data, you will encounter memory crash since the number of feasures is very big. Therefore, a dimension reduction is necessary.*\n",
    "\n",
    "- We use PCA to reduce the feasure dimension.\n",
    "\n",
    "- For now set the reduction factor to .005 to reduce the runing time until you make sure your code works. Then set the reduction factor to .03 to see if that improves the model performance.\n",
    "\n",
    "- Run the following cell to compute the shrunk data set. We use thePCA model built in Scikit-Learn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=int(.03*feature_dim))\n",
    "train_data_shrunk = pca.fit(train_data['data'].todense()).transform(train_data['data'].todense())\n",
    "test_data_shrunk = pca.transform(test_data['data'].todense())\n",
    "\n",
    "deducted_feature_dim = train_data_shrunk.shape[1]\n",
    "deducted_feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In following cell we are going to compute LDA parameters based on the formulas in (4.36), (4.37) and (4.38). The difference here is that we are not going to compute the denominator in (3.38) since that is not going to change the class score. Please make sure you have understood this. We should compute following parameters:*\n",
    "\n",
    "- mu: The mean of feasures in each class. \n",
    "\n",
    "- Sigma: The covariance matrix of each class. Be aware that in LDA we assumed that the covariance of classes are the same. Therefore, in real application we take the average of the covariance of all classes.\n",
    "\n",
    "- Pi: The class prior.\n",
    "\n",
    "- Sigma_ave: The average of covariance matrices of all classes.\n",
    "\n",
    "- beta: The parameter given in (4.37)\n",
    "\n",
    "- gamma: The parameter given in (4.36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "mu = np.zeros([train_data_shrunk.shape[1],len(categories)], dtype=float)\n",
    "Sigma = np.zeros([train_data_shrunk.shape[1],train_data_shrunk.shape[1],len(categories)])\n",
    "Pi = np.zeros(len(categories),dtype=float)\n",
    "\n",
    "for i in range(0,len(categories)):\n",
    "    same_class_data=[]\n",
    "    count = 0\n",
    "    for j in range(0,len(train_data_shrunk)):\n",
    "        if train_data['target'][j] ==i:\n",
    "            count += 1\n",
    "            mu[:,i] += train_data_shrunk[j]\n",
    "            same_class_data.append(train_data_shrunk[j])\n",
    "    Pi[i]= count/train_data_shrunk.shape[0]\n",
    "    mu[:,i] /= count\n",
    "    \n",
    "    temp= np.zeros([count,train_data_shrunk.shape[1]],dtype = float)\n",
    "    for t in range(0,count):\n",
    "        temp[t] = same_class_data[t]\n",
    "    Sigma[:,:,i] = np.cov(temp.T)\n",
    "    \n",
    "    \n",
    "Sigma_ave = Sigma.mean(axis=2)\n",
    "beta = np.linalg.inv(Sigma_ave).dot(mu)\n",
    "\n",
    "gamma = []\n",
    "for i in range(0, len(categories)):\n",
    "    temp = -0.5*mu[:,i].T.dot(np.linalg.inv(Sigma_ave).dot(mu[:,i])) + np.log(Pi[i])\n",
    "    gamma.append(temp)\n",
    "gamma=np.array(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the following cell we are going to compute:*\n",
    "- class_scores: The class score of each data point in the test data set. This is the numerator in (4.38) \n",
    "- class_prediction: The predicted class based on the class score which is going to be the maximum of all scores of classes.\n",
    "- accuracy_rate: The rate of accurracy of our LDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965262379896526"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_scores = []\n",
    "for i in range(0,len(test_data_shrunk)):\n",
    "    scores = np.exp(beta.T.dot(test_data_shrunk[i])+gamma)/sum(np.exp(beta.T.dot(test_data_shrunk[i])+gamma))\n",
    "    class_scores.append(scores)\n",
    "class_scores = np.array(class_scores)\n",
    "    \n",
    "class_prediction = []\n",
    "for i in range(0,class_scores.shape[0]):\n",
    "    predict = class_scores[i].tolist().index(max(class_scores[i]))\n",
    "    class_prediction.append(predict)\n",
    "class_prediction = np.array(class_prediction)\n",
    "\n",
    "correct = np.sum(class_prediction == test_data['target']) \n",
    "accuracy_rate = float(correct)/test_data_shrunk.shape[0]\n",
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Compare with Other Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In following cell we are going to compare LDA with other linear models built in SciKit-Learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462675535846268"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=4)\n",
    "neigh.fit(train_data_shrunk,train_data['target']) \n",
    "(abs(neigh.predict(test_data_shrunk)-test_data['target'])==0).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
